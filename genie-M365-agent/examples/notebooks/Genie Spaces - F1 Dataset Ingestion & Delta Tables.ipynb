{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca0180a-34b3-4006-9a1a-9cddeec36d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Formula 1 (1950–2024) — GitHub ZIP → Unity Catalog (Delta)\n",
    "**Goal:** Download the F1 dataset from a GitHub ZIP, store it in a Unity Catalog **Volume**, and auto-ingest each CSV into **Delta tables** for SQL/Genie.\n",
    "\n",
    "**Stack:** Databricks Serverless · Unity Catalog · UC Volumes · Delta Lake · Python + Bash\n",
    "\n",
    "Kaggle F1 Dataset: [Link](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "093deebd-0e98-4b01-9148-771ca4224c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1) Prerequisites & Parameters\n",
    "- Permission to create/use a **UC catalog, schema, and volume**.\n",
    "- A valid **ADLS Gen2** path for the catalog **MANAGED LOCATION**.\n",
    "- Source ZIP: **GitHub** (no Kaggle login needed).\n",
    "\n",
    "We’ll:\n",
    "1) Create (or reuse) catalog/schema/volume  \n",
    "2) Download & unzip into the Volume  \n",
    "3) Ingest all CSVs (recursively) → **Delta tables** in Unity Catalog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11dbdee7-a9af-411e-99b2-7c8c08494010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2) Configuration (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b1cb6d-c822-4093-aa14-6be37518f2b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration: change these if needed ---\n",
    "\n",
    "CATALOG = \"_databricks_demos\"          # UC catalog\n",
    "SCHEMA  = \"genie_data\"                 # UC schema (aka database)\n",
    "VOLUME  = \"f1_data\"                    # UC volume name\n",
    "\n",
    "# Managed location for the new catalog (ADLS Gen2 URI)\n",
    "# ⚠️ ATENTION: Change it to your case\n",
    "CATALOG_MANAGED_LOCATION = \"abfss://<your_container>@<your_storage>.dfs.core.windows.net/\"\n",
    "\n",
    "# Source ZIP (your GitHub archive)\n",
    "ARCHIVE_URL = \"https://github.com/ArnoldSouza/databricks-genie-teams-copilot-m365/raw/refs/heads/main/examples/data/archive.zip\"\n",
    "\n",
    "# Where files will be placed inside the UC Volume\n",
    "DEST_DIR = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "RAW_DIR  = f\"{DEST_DIR}/raw\"   # we’ll unzip here\n",
    "\n",
    "print(\"Configured paths:\")\n",
    "print(f\"  Catalog.Schema.Volume : {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "print(f\"  Volume root dir       : {DEST_DIR}\")\n",
    "print(f\"  Raw unzip dir         : {RAW_DIR}\")\n",
    "print(f\"  ZIP source            : {ARCHIVE_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf00f76d-ee93-4aa5-91ad-bcd3b41b6701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3) Create UC Catalog, Schema, and Volume (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0451235-2561-4581-8508-477e8f4ac462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create catalog, schema, and volume. Then set the current context.\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE CATALOG IF NOT EXISTS {CATALOG}\n",
    "  MANAGED LOCATION '{CATALOG_MANAGED_LOCATION}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE {SCHEMA}\")\n",
    "\n",
    "display(spark.sql(\"SHOW VOLUMES\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b531cc35-3bdd-4a5a-9f86-8a96cf020640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download & Unzip from GitHub (Bash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92044dc-9274-4e99-bade-dbad6f6b2135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "# --- Download the GitHub ZIP and unzip into the UC Volume ---\n",
    "URL=\"https://github.com/ArnoldSouza/databricks-genie-teams-copilot-m365/raw/refs/heads/main/examples/data/archive.zip\"\n",
    "DEST_DIR=\"/Volumes/_databricks_demos/genie_data/f1_data\"\n",
    "RAW_DIR=\"${DEST_DIR}/raw\"\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"Creating target dirs...\"\n",
    "mkdir -p \"$RAW_DIR\"\n",
    "\n",
    "echo \"Downloading ZIP...\"\n",
    "# Use a stable file name\n",
    "ZIP_PATH=\"${RAW_DIR}/archive.zip\"\n",
    "wget -q -O \"$ZIP_PATH\" \"$URL\"\n",
    "\n",
    "echo \"Unzipping...\"\n",
    "# -o to overwrite; -d to target; -q for quieter output (remove -q if you want details)\n",
    "unzip -o -q \"$ZIP_PATH\" -d \"$RAW_DIR\"\n",
    "\n",
    "echo \"Cleaning up ZIP...\"\n",
    "rm -f \"$ZIP_PATH\"\n",
    "\n",
    "echo \"Files extracted to $RAW_DIR:\"\n",
    "# Show a limited listing for readability\n",
    "find \"$RAW_DIR\" -maxdepth 2 -type f | sed -n '1,200p'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d01a40-3cda-42f4-9a3a-a5b9bcd71156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5) Quick Audit (Bash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df4a196-9483-4245-8a37-11b4cf0a1d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "RAW_DIR=\"/Volumes/_databricks_demos/genie_data/f1_data/raw\"\n",
    "echo \"CSV files found (first 200 lines of listing):\"\n",
    "find \"$RAW_DIR\" -type f -name \"*.csv\" | sed -n '1,200p'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8663748-a606-405f-a458-f2e0ad321e54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6) Ingest All CSVs → Delta Tables (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed77f34-4850-4b9e-bf0a-e1b556a6cff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest all CSVs → Delta tables\n",
    "\n",
    "import os, re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "RAW_DIR = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/raw\"\n",
    "\n",
    "def sanitize_table_name(path: str) -> str:\n",
    "    base = os.path.splitext(os.path.basename(path))[0]\n",
    "    base = re.sub(r\"[^a-z0-9_]+\", \"_\", base.lower()).strip(\"_\")\n",
    "    return base\n",
    "\n",
    "# Recursively collect CSVs\n",
    "csv_paths = []\n",
    "for root, _, files in os.walk(RAW_DIR):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".csv\"):\n",
    "            csv_paths.append(os.path.join(root, f))\n",
    "\n",
    "if not csv_paths:\n",
    "    raise FileNotFoundError(f\"No CSV files found under {RAW_DIR}\")\n",
    "\n",
    "created = []\n",
    "for path in sorted(csv_paths):\n",
    "    tbl  = sanitize_table_name(path)\n",
    "    fqtn = f\"{CATALOG}.{SCHEMA}.{tbl}\"\n",
    "\n",
    "    df = (spark.read.format(\"csv\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .option(\"multiLine\", \"true\")\n",
    "          .option(\"escape\", '\"')\n",
    "          .load(path)\n",
    "          .withColumn(\"_ingest_file\", F.col(\"_metadata.file_path\"))\n",
    "          .withColumn(\"_ingest_ts\",   F.current_timestamp())\n",
    "          .withColumn(\"_source_modified\", F.col(\"_metadata.file_modification_time\"))\n",
    "         )\n",
    "\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"overwriteSchema\", \"true\")\n",
    "       .saveAsTable(fqtn))\n",
    "\n",
    "    spark.sql(f\"COMMENT ON TABLE {fqtn} IS 'F1 GitHub ZIP import; auto-generated'\")\n",
    "\n",
    "    created.append((fqtn, df.count()))\n",
    "\n",
    "print(\"Created/overwritten tables:\")\n",
    "for fqtn, n in created:\n",
    "    print(f\"  - {fqtn} (rows: {n})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f38d02e-2d46-46ec-9222-b329641e7e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7) Test one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefeb346-4a04-4553-8bb4-47715b8213ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from _databricks_demos.genie_data.circuits limit 10"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8473336422154784,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Genie Spaces - F1 Dataset Ingestion & Delta Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}